{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment-02, Probability Model A First Look: An Introduction of Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "1. Review the course online programming code; \n",
    "2. Review the main questions; \n",
    "3. Using wikipedia corpus to build a language model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Review the course online programming code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.107 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#@author:yangsong\n",
    "\n",
    "database='/home/yangsong/下载/export_sql_1558435/sqlResult_1558435.csv'\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "os.path.exists(database) #判断路径合法性\n",
    "'''\n",
    "读取数据库文件，并提取content列转成数组形式。\n",
    "GB18030是最新编码方式，从所包含的中文字符个数来说:GB2312 < GBK < GB18030\n",
    "'''\n",
    "dataframe=pd.read_csv(database,encoding='GB18030')\n",
    "all_articles=dataframe['content'].tolist()\n",
    "\n",
    "# print (all_articles[:3])\n",
    "'''\n",
    "print ('aaaa \\r\\n bbbbbb')\n",
    "print ('ccccc \\n ddddddd')\n",
    "print ('eeeeeeeee \\r ffffff')\n",
    "输出为：\n",
    "aaaa\n",
    " bbbbbb\n",
    "ccccc\n",
    " ddddddd\n",
    " ffffff\n",
    "首先输出后行首会空一格。其次若处理不当保留/r，会将本字符串前字符覆盖，\\r相当于return后面字符。\n",
    "'''\n",
    "def token(strings):\n",
    "    #根据文中的内容格式，我觉得保留文中空格对分词会有帮助。同时，我将换行符替换为空格形式。\n",
    "    strings=strings.replace('\\r\\n',' ').replace('\\n',' ')\n",
    "    res=' '.join(re.findall('[\\w|\\d]+', strings))\n",
    "    return res\n",
    "all_articles=[token(str(a)) for a in all_articles]\n",
    "text=''\n",
    "for a in all_articles:\n",
    "    text+=a\n",
    "# print ('length of text:{}'.format(len(text)))\n",
    "TEXT=text\n",
    "from functools import reduce\n",
    "\n",
    "# py3以后使用，必须导入 from functools import reduce\n",
    "# reduce函数的定义：reduce(function, sequence[, initial]) -> value\n",
    "# function参数是一个有两个参数的函数，reduce依次从sequence中取一个元素\n",
    "# 和上一次调用function的结果做参数再次调用function\n",
    "# 第一次调用function时，如果提供initial参数\n",
    "# 会以sequence中的第一个元素和initial作为参数调用function\n",
    "# 否则会以序列sequence中的前两个元素做参数调用function\n",
    "\n",
    "txt_from_reduce=reduce(lambda x,y:x+y,all_articles)\n",
    "import  jieba\n",
    "def cut(strings):return list(jieba.cut(strings)) #jieba 返回生成器需要转成list使用\n",
    "ALL_TOKENS = cut(TEXT)\n",
    "valida_tokens = [t for t in ALL_TOKENS if t.strip()]\n",
    "print (ALL_TOKENS)\n",
    "print (valida_tokens)\n",
    "#Get the frequences of words\n",
    "from collections import Counter\n",
    "\n",
    "word_count=Counter(valida_tokens)\n",
    "print (word_count.most_common(10))\n",
    "frequences=[f for f,w in words_count.most_common(100) ]\n",
    "x=[i for i in range(len(frequences[:100]))]\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline #jupyter中调取plot()绘图功能，在console中生成图像\n",
    "plt.plot(x,frequences)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(x,np.log(frequences))\n",
    "frequences_all=[f for f,w in word_count.most_common()]\n",
    "frequences_sum=sum(frequences_all)\n",
    "\n",
    "#单个词频\n",
    "def get_prob(word):\n",
    "    esp=1/frequences_sum\n",
    "    if word in words_count:\n",
    "        return words_count[word]/frequences_sum\n",
    "    else:\n",
    "        return esp\n",
    "#连乘\n",
    "def profuct(nums):\n",
    "    return reduce(lambda n1,n2:n1*n2,nums)\n",
    "\n",
    "#判断句子出现概率\n",
    "def language_model_one_gram(strings):\n",
    "    words=cut(strings)\n",
    "    return profuct([get_prob(w) for w in words])\n",
    "#以下举例\n",
    "language_model_one_gram('广交会下个月举办')\n",
    "language_model_one_gram('长征火箭下周发射')\n",
    "language_model_one_gram('长征火箭下周发射')\n",
    "sentences = \"\"\"\n",
    "这是一个比较正常的句子\n",
    "这个一个比较罕见的句子\n",
    "小明毕业于清华大学\n",
    "小明毕业于秦华大学\n",
    "\"\"\".split()\n",
    "for s in sentences:\n",
    "    print(s, language_model_one_gram(s))\n",
    "need_compared = [\n",
    "    \"今天晚上请你吃大餐，我们一起吃日料 明天晚上请你吃大餐，我们一起吃苹果\",\n",
    "    \"真事一只好看的小猫 真是一只好看的小猫\",\n",
    "    \"我去吃火锅，今晚 今晚我去吃火锅\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 = language_model_one_gram(s1), language_model_one_gram(s2)\n",
    "\n",
    "    better = s1 if p1 > p2 else s2\n",
    "\n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-' * 4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-' * 4 + ' {} with probility {}'.format(s2, p2))\n",
    "\n",
    "# 2_Gram\n",
    "valida_tokens=[str(t) for t in valida_tokens]\n",
    "all_2_grams_words = [''.join(valid_tokens[i:i+2]) for i in range(len(valid_tokens[:-2]))]\n",
    "_2_gram_sum=len(all_2_gram_words)\n",
    "_2_gram_counter=Counter(all_2_gram_words) #所有两个词语结合词频-list\n",
    "def get_combination_prob(w1,w2):\n",
    "    if w1+w2 in all_2_grams_words:\n",
    "        return _2_gram_counter[w1+w2]/_2_gram_sum\n",
    "    else:\n",
    "        1/_2_gram_sum\n",
    "def get_prob_2_gram(w1,w2):\n",
    "    return get_combination_prob(w1,w2)/get_prob(w1)\n",
    "def language_model_2_gram(sentence):\n",
    "    words=cut(sentence)\n",
    "    for k,v in enumerate(words):\n",
    "        if k==0:\n",
    "            prob=get_prob(v)\n",
    "        else:\n",
    "            previous=words[k-1]\n",
    "            prob=get_combination_prob(previous,v)\n",
    "        sentence_probility*=prob\n",
    "    return sentence_probility\n",
    "#Review the problem using 2-gram\n",
    "need_compared = [\n",
    "    \"今天晚上请你吃大餐，我们一起吃日料 明天晚上请你吃大餐，我们一起吃苹果\",\n",
    "    \"真事一只好看的小猫 真是一只好看的小猫\",\n",
    "    \"今晚我去吃火锅 今晚火锅去吃我\",\n",
    "    \"洋葱奶昔来一杯 养乐多绿来一杯\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 = langauge_model_of_2_gram(s1), langauge_model_of_2_gram(s2)\n",
    "\n",
    "    better = s1 if p1 > p2 else s2\n",
    "\n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-' * 4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-' * 4 + ' {} with probility {}'.format(s2, p2))\n",
    "\n",
    "grammar = \"\"\"\n",
    "sentence => noun_phrase verb_phrase \n",
    "noun_phrase => Article Adj* noun belong \n",
    "belong => de property\n",
    "de => 的\n",
    "property => 眼睛 | 裙子 | 胳膊 | 尾巴\n",
    "Adj* => null | Adj Adj*\n",
    "verb_phrase => verb noun_phrase\n",
    "Article =>  一个 | 这个\n",
    "noun =>   女人 |  篮球 | 桌子 | 小猫\n",
    "verb => 看着   |  坐在 |  听着 | 看见\n",
    "Adj =>   蓝色的 |  好看的 | 小小的\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def parse_grammar(grammar_str, sep='=>'):\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "\n",
    "        target, rules = line.split(sep)\n",
    "\n",
    "        grammar[target.strip()] = [r.split() for r in rules.split('|')]\n",
    "\n",
    "    return grammar\n",
    "\n",
    "\n",
    "def gene(grammar_parsed, target='sentence'):\n",
    "    if target not in grammar_parsed: return target\n",
    "\n",
    "    rule = random.choice(grammar_parsed[target])\n",
    "    return ''.join(gene(grammar_parsed, target=r) for r in rule if r != 'null')\n",
    "g = parse_grammar(grammar)\n",
    "random_generated = [gene(g) for _ in range(100)]\n",
    "\n",
    "sorted(random_generated, key=langauge_model_of_2_gram, reverse=True)\n",
    "\n",
    "#-----------------正则表达式及爬虫练习----------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review the main points of this lesson. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. How to Github and Why do we use Jupyter and Pycharm; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:   \n",
    "**一、how to github**  \n",
    "1.配置git的用户，用来标识作者的身份  \n",
    "    git config --global user.name <name>  \n",
    "    git config --global user.email <email>  \n",
    "2.配置电脑的sshkey  \n",
    "    相关命令：ssh-keygen -t rsa -C \"<youemail>\"\n",
    "  配置好以后，在GitHub账户配置中进行关联  \n",
    "3.初次提交  \n",
    "    git init //把这个目录变成Git可以管理的仓库  \n",
    "    git add README.md //文件添加到仓库  \n",
    "    git add <filepath>  \n",
    "    git commit -m <your commit> //提交注释  \n",
    "    git remote add origin <projectlink> //关联远程仓库  \n",
    "    git push -u origin master //把本地库的所有内容推送到远程库上  \n",
    "4.后面提交  \n",
    "    git pull <projectlink>//更新项目  \n",
    "    git add .//添加文件  \n",
    "    git commit -m \"update test\" //提交注释  \n",
    "    git push -u origin master //将项目推到远程  \n",
    "**二、jupyter and Pycharm**  \n",
    " jupyter 相当于科学计算的笔记作用，可辅助运行Python，matlib等工具，pycharm作为项目开发使用，可进行debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 概率模型是描述了一个或多个随机变量的之间的概率事件的集合,通过概率集合组合成的数学模型叫做概率模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:天气预报、总统选举、交通流量预测、自然语言处理-人机交互、股市预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:通过概率我们可以做出可能性最大的决策。难点：解析的准确性，不能全部理解句子表达的内容，句子有时会结合场景进行分析。同时，对于语料之外的内容我们无法正确解析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 语言模型一种描述词语、短语或句子相似度的概率模型，它整体的数据集为预先设定好的一个语料集合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  6. Can you came up with some sceneraies at which we could use Language Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:主要为聊天机器人，垃圾邮件过滤，内容推荐（可能）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:是指单个词语出现在语料库中的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:通过单个词语判断的话，无法确定词语先后的连贯性，会导致词不达意问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.  What't the 2-gram models; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:是指两个词语，按照两个词语出现的先后顺序，形成的概率模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. what's the web crawler, and can you implement a simple crawler? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------微博热搜榜-----------------\n",
      "  名次            关键词                   热度  \n",
      "   1      唐嫣胖了                 2563388\n",
      "   2      萧亚轩ins更新             1688860\n",
      "   3      吴磊的24个房间             1545809\n",
      "   4      视觉中国道歉               1356184\n",
      "   5      晨阳 我可以               1284468\n",
      "   6      于慧坑向真                1090572\n",
      "   7      姜潮麦迪娜结婚了             1004804\n",
      "   8      66万买奔驰还没开就漏油         527197\n",
      "   9      视觉中国3.88亿股解禁         494189\n",
      "   10     共青团中央 视觉中国           464240\n",
      "   11     955白名单九成是外企          460207\n",
      "   12     长得胖却漂亮的女生            459885\n",
      "   13     孔孝真被韩国税厅调查           458181\n",
      "   14     半年发一次朋友圈的人           451343\n",
      "   15     桃子千层蛋糕               428689\n",
      "   16     菲律宾发现新人类物种           406173\n",
      "   17     许玮甯手写道歉信             406149\n",
      "   18     程潇妆容                 395271\n",
      "   19     溜肩女孩的烦恼              387752\n",
      "   20     2000斤皮皮虾车祸中丧生        380203\n",
      "   21     北京奥运会把工人名字刻在钢筋上      372095\n",
      "   22     黄子韬和动漫人物撞脸           352367\n",
      "   23     肯德基老爷爷换形象了           351705\n",
      "   24     美国确诊超600例超级真菌        335565\n",
      "   25     妞妞舅舅                 318510\n",
      "   26     想吃炸鸡的兵哥哥             278080\n",
      "   27     记忆中最虐心的cp            246016\n",
      "   28     广州冰雹                 237447\n",
      "   29     JDG老板 微笑             230382\n",
      "   30     青春斗                  209573\n",
      "   31     沈阳苏家屯车祸              194370\n",
      "   32     聊天终结语句               181869\n",
      "   33     坐副驾驶考驾照其实是教练         169085\n",
      "   34     视觉中国网站无法打开           167033\n",
      "   35     草莓白巧克力梳乎厘            155607\n",
      "   36     妻子的浪漫旅行              152436\n",
      "   37     深圳天气                 151479\n",
      "   38     节目组回应蒋方舟谈热巴立人设       151330\n",
      "   39     吃不腻的特色美食             150603\n",
      "   40     什么是妈妈粉               143226\n",
      "   41     NBA巨星们的朋友圈           139406\n",
      "   42     证券业从业者流失1.1万         131750\n",
      "   43     有个有钱的追星姐妹            131144\n",
      "   44     日本人的卫生间细节            128875\n",
      "   45     日托养老院                123973\n",
      "   46     幼儿园老师要求学生拍自家车        122254\n",
      "   47     贝弗利指导克劳德防守哈登         122184\n",
      "   48     恋爱中女生最爱撒的谎           122166\n",
      "   49     帮公司签字贷款欠下上亿          121429\n",
      "   50     童模基地                 117431\n"
     ]
    }
   ],
   "source": [
    "#新浪热搜榜爬虫练习\n",
    "import requests\n",
    "import re\n",
    "url='https://s.weibo.com/top/summary?cate=realtimehot'\n",
    "page=requests.get(url).text\n",
    "print('-----------------微博热搜榜-----------------')\n",
    "print('  名次  '+'          关键词          '+'         热度  ')\n",
    "# print (page)\n",
    "# tbody_pattern=re.compile(u'<table class=\"list-table\">(.*?)</table>')\n",
    "#findall若要匹配多行，需添加参数re.S，可匹配到换行符\n",
    "tbody=re.findall(u'<tbody>(.*?)</tbody>',page,re.S)\n",
    "trs=re.findall(u'<tr class=\"\">(.*?)</tr>',page,re.S)\n",
    "trs.pop(0)#去掉了第一个置顶热搜不处理\n",
    "\n",
    "for t in trs:\n",
    "    tds=re.findall(u'<td.*?>(.*?)</td>',t,re.S)\n",
    "    rank=tds[0] #名次\n",
    "    keywords=re.findall(u'<a.*?>(.*?)</a>',tds[1],re.S)[0]#关键词\n",
    "    hot=re.findall(u'<span>(\\d+)</span>',tds[1],re.S)[0]#热搜榜\n",
    "    #输出格式处理\n",
    "    rank='   '+rank+' '*(6-len(rank))\n",
    "    keywords=keywords+' '*(20-len(keywords))\n",
    "    print (rank,keywords,hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11.  There may be some issues to make our crwaler programming difficult, what are these, and how do we solve them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "1.采集的网站较大，单个机器需要几个月采集---->通过分布式爬虫采集  \n",
    "2.某些网站采集时间长，或请求过快会封IP----->采用代理IP池，请求不成功更换IP  \n",
    "3.网页数据通过ajax加载------>通过网页检查器，查看网络请求过程  \n",
    "4.ajax请求链接不连续---->网站可能每次请求链接有固定算法，有时根据时间戳、用户IP、用户头文件等加密方式，需要观察找规律  \n",
    "5.网站需要登录访问---->通过插件框架模拟访问"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. What't the Regular Expression and how to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:通过一定的句法规则对文本中内容进行检索、提取、替换等操作，帮助从文本中获取想要的内容。  \n",
    "Python中需要import re,常用的功能包括：match、search、findall、compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Wikipedia dataset to finish the language model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: You need to download the corpus from wikipedis:\n",
    "> https://dumps.wikimedia.org/zhwiki/20190401/\n",
    "\n",
    "Step 2: You may need the help of wiki-extractor:\n",
    "\n",
    "> https://github.com/attardi/wikiextractor\n",
    "\n",
    "Step 3: Using the technologies and methods to finish the language model; \n",
    "\n",
    "Step 4: Try some interested sentence pairs, and check if your model could fit them\n",
    "\n",
    "Step 5: If we need to solve following problems, how can language model help us? \n",
    "\n",
    "+ Voice Recognization.\n",
    "+ Sogou *pinyin* input.\n",
    "+ Auto correction in search engibbne.\n",
    "+ Abnormal Detection.  \n",
    "根据前后词语关系帮助识别错误的输入，当大于一定概率时做出错误提示或者修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#@author:yangsong\n",
    "import os\n",
    "import jieba\n",
    "import time\n",
    "import re\n",
    "from hanziconv import HanziConv\n",
    "from collections import Counter\n",
    "ROOTDIR='D:\\GitHub\\les2\\words'\n",
    "words_counter_path='D:\\GitHub\\les2\\words\\words_counter.txt'\n",
    "_2_words_count_path='D:\\GitHub\\les2\\words\\_2_words_counter.txt'\n",
    "total_words='D:\\GitHub\\les2\\words\\\\total_words.txt'\n",
    "\n",
    "def process_words(ROOTDIR):\n",
    "    #分别获取单个词语和两个词语的词频统计\n",
    "    _2_words_counter=Counter()\n",
    "    res_contents=''\n",
    "    for lists in os.listdir(ROOTDIR):\n",
    "        path = os.path.join(ROOTDIR, lists)\n",
    "        with open(path,'r', encoding='UTF-8') as file_object:\n",
    "            content=file_object.read()\n",
    "            format_contents = content.replace(' ', '').replace('\\n','')\n",
    "            format_contents=re.sub(r'<.*?>', '', format_contents)\n",
    "            format_contents=re.sub('[+——() ? 【】“”！，：。？、~@#￥%……&*（）《 》]+', '',format_contents)\n",
    "            res_contents +=HanziConv.toSimplified(format_contents)\n",
    "    with open(total_words, 'a+', encoding='UTF-8') as f:\n",
    "        f.write(res_contents)\n",
    "    print(\"write process is over\")\n",
    "process_words(ROOTDIR)\n",
    "def get_count(path,gram):\n",
    "    with open(path ,'r', encoding='UTF-8') as f:\n",
    "        format_contents=f.read()\n",
    "        words_list = list(jieba.cut(format_contents))\n",
    "        if gram==1:\n",
    "            words_counter= Counter(words_list)  # 将文本内容做词频统计累加，单个词语\n",
    "            totals = [v for k, v in words_counter.items()]\n",
    "            total = sum(totals)\n",
    "            return  words_counter,total\n",
    "        else:\n",
    "            _2_words_list = [''.join(words_list[i:i + 2]) for i in range(len(words_list))]\n",
    "            _2_words_counter = Counter(_2_words_list)  # 将文本内容做词频统计累加，两个词语\n",
    "            totals = [v for k, v in _2_words_counter.items()]\n",
    "            total = sum(totals)\n",
    "            return _2_words_counter,total\n",
    "\n",
    "words_count,total=get_count(total_words,1) #单词词语的词频和总数\n",
    "_2_words_count,_2_total=get_count(total_words,2) #两个词语的词频\n",
    "\n",
    "def get_prob(word):\n",
    "    if word in words_count:\n",
    "        return words_count[word]/total\n",
    "    else:\n",
    "       return  1/total\n",
    "\n",
    "def get_combination_pro(w1,w2):\n",
    "    if w1+w2 in _2_words_count:\n",
    "        return _2_words_count[w1+w2]/_2_total\n",
    "    else:\n",
    "        return 1/_2_total\n",
    "def language_2_gram_model(sentence):\n",
    "    words=list(jieba.cut(sentence))\n",
    "    sentence_prob=1\n",
    "    for k,v in enumerate(words):\n",
    "        if k==0:\n",
    "            _2_gram_prob=get_prob(v)\n",
    "        else:\n",
    "            prob=get_combination_pro(words[k-1],v)\n",
    "            _2_gram_prob=prob/get_prob(words[k-1])\n",
    "        sentence_prob*=_2_gram_prob\n",
    "    return sentence_prob\n",
    "test_sentence=[\n",
    "    ('小明早上去买菜','小明早上去买飞机'),('让我们去吃早饭','让我们去吃地铁'),('小强正在下班回家的路上','小强正在炒菜的路上'),('吸烟有害健康','喝牛奶有害健康'),('阳光照在大地上','阳关照在大地上'),\n",
    "               ]\n",
    "for k in test_sentence:\n",
    "    print(\"----------------------------------------------\")\n",
    "    s0_prob=language_2_gram_model(k[0])\n",
    "    s1_prob = language_2_gram_model(k[1])\n",
    "    print(k[0] + '----->' + str(s0_prob) )\n",
    "    print(k[1] + '----->' + str(s1_prob) )\n",
    "    if s0_prob>s1_prob:\n",
    "        print(k[0]+'----'+'更加合理')\n",
    "    else:\n",
    "        print(k[1]+'----'+'更加合理')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compared to the previous learned parsing and pattern match problems. What's the advantage and disavantage of Probability Based Methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 基于概率的方法，参考基于大量现有资源，来判断事件发生的可能性，而现有资源也可能局限了概率的准确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional)  How to solve *OOV* problem?\n",
    "\n",
    "If some words are not in our dictionary or corpus. When we using language model, we need to overcome this `out-of-vocabulary`(OOV) problems. There are so many intelligent man to solve this probelm. \n",
    "\n",
    "-- \n",
    "\n",
    "The first question is: \n",
    "\n",
    "**Q1: How did you solve this problem in your programming task?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the sencond question is: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2: Read about the 'Turing-Good Estimator', can explain the main points about this method, and may implement this method in your programming task**\n",
    "\n",
    "Reference: \n",
    "+ https://www.wikiwand.com/en/Good%E2%80%93Turing_frequency_estimation\n",
    "+ https://github.com/Computing-Intelligence/References/blob/master/NLP/Natural-Language-Processing.pdf, Page-37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> coding in here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
